{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import threading\n",
    "import functools\n",
    "import requests\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "from src.dataset import Cifar10DataManager\n",
    "from src.training import run_training_sweep, train_epoch, validate\n",
    "from src.model import build_model\n",
    "from src.utils import get_config\n",
    "\n",
    "# Credentials from .env\n",
    "cfg = get_config()\n",
    "WANDB_API_KEY = cfg[\"WANDB_API_KEY\"]\n",
    "PROJECT_NAME = cfg[\"PROJECT_NAME\"]\n",
    "ENTITY = cfg[\"ENTITY\"]\n",
    "\n",
    "wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(f\"Environment configured for {PROJECT_NAME} ({ENTITY}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c6624",
   "metadata": {},
   "source": [
    "## Step 1: Data Versioning\n",
    "\n",
    "We download the data and implement the \"3-way Split\":\n",
    "\n",
    "1.  **Train:** 50,000 images\n",
    "2.  **Test:** 8,000 images (for evaluation)\n",
    "3.  **Simulation:** 2,000 images (HELD OUT for live traffic simulation)\n",
    "\n",
    "We then log this initial state as Artifact v1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925821a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = Cifar10DataManager(data_dir=\"./data\")\n",
    "\n",
    "# Download and split\n",
    "_ = dm.prepare_initial_split()\n",
    "\n",
    "# Log Artifact (Dataset v1)\n",
    "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"data_preparation\", name=\"cifar10_v1\")\n",
    "dataset_artifact = wandb.Artifact(\n",
    "    name=\"cifar10_dataset\",\n",
    "    type=\"dataset\",\n",
    "    description=\"CIFAR-10 Raw Data + Split Indices\"\n",
    ")\n",
    "\n",
    "dataset_artifact.add_dir(\"./data\")\n",
    "run.log_artifact(dataset_artifact)\n",
    "run.finish()\n",
    "\n",
    "print(\"Step 1 Complete: Dataset v1 logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a0240",
   "metadata": {},
   "source": [
    "## Step 2: Experimentation & Training (Hyperparameter Sweep)\n",
    "\n",
    "We run a Bayesian optimization sweep to find the best model.\n",
    "\n",
    "- **Architecture Options:** Standard, Upsample (Option A), Modified (Option B).\n",
    "- **Optimizers:** SGD, Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e15099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Data Artifact (required for training)\n",
    "prep_run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"training_prep\")\n",
    "prep_run.use_artifact(f'{ENTITY}/{PROJECT_NAME}/cifar10_dataset:latest', type='dataset').download(root=\"./data\")\n",
    "prep_run.finish()\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_acc', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'learning_rate': {'min': 0.001, 'max': 0.1},\n",
    "        'batch_size': {'values': [64, 128]},\n",
    "        'optimizer': {'values': ['adam', 'sgd']},\n",
    "        'architecture_option': {'values': ['standard', 'upsample', 'modified']},\n",
    "        'epochs': {'value': 5}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME, entity=ENTITY)\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "\n",
    "# Run Agent\n",
    "train_func = functools.partial(run_training_sweep, data_dir=\"./data\")\n",
    "wandb.agent(sweep_id, train_func, count=5, project=PROJECT_NAME, entity=ENTITY)\n",
    "\n",
    "# Save Best Config\n",
    "api = wandb.Api()\n",
    "best_run = api.sweep(f\"{ENTITY}/{PROJECT_NAME}/{sweep_id}\").best_run()\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "with open(\"artifacts/best_config.json\", \"w\") as f:\n",
    "    json.dump(best_run.config, f)\n",
    "\n",
    "print(\"Step 2 Complete: Sweep finished and best config saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5186f6",
   "metadata": {},
   "source": [
    "## Step 3: Evaluation\n",
    "\n",
    "Visualizing the performance of the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339876f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"evaluation\")\n",
    "\n",
    "# Download Data Artifact\n",
    "run.use_artifact(f'{ENTITY}/{PROJECT_NAME}/cifar10_dataset:latest', type='dataset').download(root=\"./data\")\n",
    "\n",
    "# Resolve Best Model\n",
    "api = wandb.Api()\n",
    "sweeps = api.project(PROJECT_NAME, entity=ENTITY).sweeps()\n",
    "sweep_id = sweeps[0].id if sweeps else None\n",
    "best_run = api.sweep(f\"{ENTITY}/{PROJECT_NAME}/{sweep_id}\").best_run()\n",
    "config = best_run.config\n",
    "\n",
    "# Download Model Artifact\n",
    "model_dir = best_run.logged_artifacts()[0].download(root=\"./models\")\n",
    "model_path = glob.glob(os.path.join(model_dir, \"*.pth\"))[0]\n",
    "\n",
    "# Build Test Loader (fixed test split)\n",
    "dm = Cifar10DataManager(data_dir=\"./data\")\n",
    "transform_list = [transforms.ToTensor(), transforms.Normalize(dm.mean, dm.std)]\n",
    "if config['architecture_option'] == 'upsample':\n",
    "    test_transform = transforms.Compose([transforms.Resize(224)] + transform_list)\n",
    "else:\n",
    "    test_transform = transforms.Compose(transform_list)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=False, transform=test_transform)\n",
    "indices_path = os.path.join(\"./data\", \"processed\", \"test_indices.npy\")\n",
    "real_test_set = Subset(test_set, np.load(indices_path))\n",
    "test_loader = DataLoader(real_test_set, batch_size=100, shuffle=False)\n",
    "\n",
    "# Evaluate\n",
    "model = build_model(config['architecture_option']).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_preds.extend(torch.max(outputs, 1)[1].cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "wandb.log({\n",
    "    \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        y_true=all_labels,\n",
    "        preds=all_preds,\n",
    "        class_names=['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    )\n",
    "})\n",
    "\n",
    "run.finish()\n",
    "print(\"Step 3 Complete: Evaluation results logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070cfd1",
   "metadata": {},
   "source": [
    "## Step 4: Deployment & Simulation (Feedback Loop)\n",
    "\n",
    "We launch a FastAPI app (background), send traffic from the \"Simulation Set\" (2k images), identify failures, and create a \"Feedback Dataset\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Data & Best Model\n",
    "prep_run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"deploy_prep\")\n",
    "prep_run.use_artifact(f'{ENTITY}/{PROJECT_NAME}/cifar10_dataset:latest').download(\"./data\")\n",
    "\n",
    "api = wandb.Api()\n",
    "sweeps = api.project(PROJECT_NAME, entity=ENTITY).sweeps()\n",
    "best_run = api.sweep(f\"{ENTITY}/{PROJECT_NAME}/{sweeps[0].id}\").best_run()\n",
    "config = best_run.config\n",
    "\n",
    "model_dir = best_run.logged_artifacts()[0].download(root=\"./models\")\n",
    "model_path = glob.glob(os.path.join(model_dir, \"*.pth\"))[0]\n",
    "prep_run.finish()\n",
    "\n",
    "# Build Model\n",
    "model = build_model(config['architecture_option']).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Transform for inference\n",
    "mean, std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "transform_list = [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
    "if config['architecture_option'] == 'upsample':\n",
    "    val_transform = transforms.Compose([transforms.Resize(224)] + transform_list)\n",
    "else:\n",
    "    val_transform = transforms.Compose(transform_list)\n",
    "\n",
    "# Simulation Data\n",
    "sim_data = Cifar10DataManager(\"./data\").get_simulation_data()\n",
    "\n",
    "# 1. Define App\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(payload: dict):\n",
    "    idx = payload.get(\"index\")\n",
    "    image, _ = sim_data[idx]\n",
    "    tensor = val_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor)\n",
    "        conf, pred = torch.max(torch.nn.functional.softmax(output, dim=1), 1)\n",
    "    return {\"prediction\": int(pred.item()), \"confidence\": float(conf.item())}\n",
    "\n",
    "# 2. Run Server\n",
    "server_thread = threading.Thread(\n",
    "    target=lambda: uvicorn.run(app, host=\"127.0.0.1\", port=8005, log_level=\"error\"),\n",
    "    daemon=True\n",
    ")\n",
    "server_thread.start()\n",
    "time.sleep(3)\n",
    "\n",
    "# 3. Simulate Traffic\n",
    "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"deployment_simulation\")\n",
    "feedback_data = []\n",
    "table = wandb.Table(columns=[\"index\", \"pred\", \"truth\", \"conf\", \"correct\"])\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "for idx in np.random.choice(len(sim_data), 30, replace=False):\n",
    "    _, gt = sim_data[idx]\n",
    "    resp = requests.post(\"http://127.0.0.1:8005/predict\", json={\"index\": int(idx)}).json()\n",
    "    pred = resp[\"prediction\"]\n",
    "    correct = (pred == gt)\n",
    "    table.add_data(idx, classes[pred], classes[gt], resp[\"confidence\"], correct)\n",
    "    if not correct:\n",
    "        feedback_data.append((int(idx), int(gt)))\n",
    "\n",
    "wandb.log({\"simulation_results\": table})\n",
    "\n",
    "if feedback_data:\n",
    "    np.save(\"feedback_v1.npy\", feedback_data)\n",
    "    art = wandb.Artifact(\"cifar10-feedback\", type=\"dataset\")\n",
    "    art.add_file(\"feedback_v1.npy\")\n",
    "    wandb.log_artifact(art)\n",
    "\n",
    "wandb.finish()\n",
    "print(\"Step 4 Complete: Feedback gathered and logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c855b",
   "metadata": {},
   "source": [
    "## Step 5: Automated Retraining\n",
    "\n",
    "We detect the new dataset version and retrain the best model config (Option B) on the updated data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdfdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"retrain\", tags=[\"retrain\"])\n",
    "\n",
    "try:\n",
    "    print(\"Downloading Feedback...\")\n",
    "    f_art = run.use_artifact(f'{ENTITY}/{PROJECT_NAME}/cifar10-feedback:latest').download(root=\".\")\n",
    "    feedback = np.load(os.path.join(f_art, \"feedback_v1.npy\"))\n",
    "except:\n",
    "    print(\"No feedback found.\")\n",
    "    feedback = []\n",
    "\n",
    "if len(feedback) > 0:\n",
    "    print(\"Downloading Baseline Data...\")\n",
    "    run.use_artifact(f'{ENTITY}/{PROJECT_NAME}/cifar10_dataset:latest').download(\"./data\")\n",
    "    \n",
    "    api = wandb.Api()\n",
    "    sweeps = api.project(PROJECT_NAME, entity=ENTITY).sweeps()\n",
    "    best_run = api.sweep(f\"{ENTITY}/{PROJECT_NAME}/{sweeps[0].id}\").best_run()\n",
    "    config = best_run.config\n",
    "    \n",
    "    print(\"Downloading Baseline Model...\")\n",
    "    m_dir = best_run.logged_artifacts()[0].download(root=\"./models\")\n",
    "    m_path = glob.glob(os.path.join(m_dir, \"*.pth\"))[0]\n",
    "    \n",
    "    # Dataset Merge\n",
    "    dm = Cifar10DataManager()\n",
    "    tf_list = [transforms.ToTensor(), transforms.Normalize(dm.mean, dm.std)]\n",
    "    train_tf = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 4)] + tf_list)\n",
    "    if config['architecture_option'] == 'upsample':\n",
    "        train_tf = transforms.Compose([transforms.Resize(224), transforms.RandomHorizontalFlip()] + tf_list)\n",
    "    \n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=False, transform=train_tf)\n",
    "    raw_sim = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=False)\n",
    "    \n",
    "    class FeedbackDS(torch.utils.data.Dataset):\n",
    "        def __init__(self, raw, inds, tf):\n",
    "             self.raw = raw; self.inds = [int(i[0]) for i in inds]; self.tf = tf\n",
    "        def __len__(self): return len(self.inds)\n",
    "        def __getitem__(self, i): \n",
    "             img, label = self.raw[self.inds[i]]\n",
    "             return self.tf(img), label\n",
    "             \n",
    "    fb_ds = FeedbackDS(raw_sim, feedback, train_tf)\n",
    "    loader = DataLoader(ConcatDataset([train_set, fb_ds]), batch_size=config['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Validation Loader\n",
    "    test_tf = transforms.Compose(tf_list)\n",
    "    if config['architecture_option'] == 'upsample':\n",
    "        test_tf = transforms.Compose([transforms.Resize(224)] + tf_list)\n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=False, transform=test_tf)\n",
    "    test_indices = np.load(os.path.join(\"./data\", \"processed\", \"test_indices.npy\"))\n",
    "    test_loader = DataLoader(Subset(test_set, test_indices), batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Retrain\n",
    "    model = build_model(config['architecture_option']).to(device)\n",
    "    model.load_state_dict(torch.load(m_path, map_location=device))\n",
    "    \n",
    "    lr = config.get('learning_rate', 0.001)\n",
    "    if config.get('optimizer') == 'sgd':\n",
    "        opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"Fine-tuning...\")\n",
    "    for e in range(5):\n",
    "        train_loss = train_epoch(model, loader, crit, opt, device)\n",
    "        val_loss, val_acc = validate(model, test_loader, crit, device)\n",
    "        print(f\"Epoch {e+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        wandb.log({\"retrain_loss\": train_loss, \"val_loss\": val_loss, \"val_acc\": val_acc, \"epoch\": e})\n",
    "        \n",
    "    torch.save(model.state_dict(), \"retrained.pth\")\n",
    "    art = wandb.Artifact(\"retrained-model\", type=\"model\")\n",
    "    art.add_file(\"retrained.pth\")\n",
    "    run.log_artifact(art)\n",
    "    print(\"Retraining Complete.\")\n",
    "else:\n",
    "    print(\"Skipping.\")\n",
    "    \n",
    "run.finish()\n",
    "print(\"Step 5 Complete: Automated retraining finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
