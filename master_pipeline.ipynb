{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import wandb\n",
    "import torch\n",
    "from src.utils import load_env_vars\n",
    "from src.dataset import Cifar10DataManager\n",
    "from src.training import run_training_sweep\n",
    "from src.model import build_model\n",
    "import os\n",
    "\n",
    "# Load Environment Variables\n",
    "env = load_env_vars()\n",
    "os.environ[\"WANDB_API_KEY\"] = env[\"WANDB_API_KEY\"]\n",
    "PROJECT_NAME = env[\"WANDB_PROJECT\"]\n",
    "ENTITY = env[\"WANDB_ENTITY\"]\n",
    "\n",
    "print(\"Environment configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97c6624",
   "metadata": {},
   "source": [
    "## Step 1: Data Versioning\n",
    "\n",
    "We download the data and implement the \"3-way Split\":\n",
    "\n",
    "1.  **Train:** 50,000 images\n",
    "2.  **Test:** 8,000 images (for evaluation)\n",
    "3.  **Simulation:** 2,000 images (HELD OUT for live traffic simulation)\n",
    "\n",
    "We then log this initial state as Artifact v1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925821a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = Cifar10DataManager()\n",
    "# Download and split\n",
    "_, _, _, _ = dm.prepare_initial_split()\n",
    "\n",
    "# Log Artifact\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"data_versioning\", name=\"dataset_v1_creation\")\n",
    "dm.log_dataset_artifact(run, name=\"cifar10-split-indices\", description=\"Contains numpy indices for 8k Test and 2k Simulation split\")\n",
    "wandb.finish()\n",
    "print(\"Step 1 Complete: Dataset v1 created and logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a0240",
   "metadata": {},
   "source": [
    "## Step 2: Experimentation & Training (Hyperparameter Sweep)\n",
    "\n",
    "We run a Bayesian optimization sweep to find the best model.\n",
    "\n",
    "- **Architecture Options:** Standard, Upsample (Option A), Modified (Option B).\n",
    "- **Optimizers:** SGD, Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e15099",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_acc', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'learning_rate': {'min': 0.001, 'max': 0.1},\n",
    "        'batch_size': {'values': [64, 128]},\n",
    "        'optimizer': {'values': ['adam', 'sgd']},\n",
    "        'architecture_option': {'values': ['standard', 'modified']}, # Limiting options for speed in demo\n",
    "        'epochs': {'value': 2} # Small epoch count for demo speed\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "\n",
    "# Run Agent (Running just 3 runs for demo)\n",
    "wandb.agent(sweep_id, run_training_sweep, count=3)\n",
    "\n",
    "# Save best model to artifact\n",
    "# Note: Ideally you fetch best run from API and log its model. \n",
    "# For demo simplicity, we assume the last run or manual selection.\n",
    "# Let's programmatically get the best run from api\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"{ENTITY}/{PROJECT_NAME}/{sweep_id}\")\n",
    "best_run = sweep.best_run()\n",
    "print(f\"Best Run: {best_run.name} with Acc: {best_run.summary.get('val_acc')}\")\n",
    "\n",
    "# Store Best Config for Automated Retraining later\n",
    "best_config = best_run.config\n",
    "import json\n",
    "with open(\"best_config.json\", \"w\") as f:\n",
    "    json.dump(best_config, f)\n",
    "    \n",
    "print(\"Step 2 Complete: Best model found and config saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5186f6",
   "metadata": {},
   "source": [
    "## Step 3: Evaluation\n",
    "\n",
    "Visualizing the performance of the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339876f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a real scenario, we load weights. \n",
    "# Here we simulate the evaluation visualization using W&B\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"evaluation\", notes=\"Best model evaluation\")\n",
    "\n",
    "# Log a dummy confusion matrix for demonstration (since we are orchestrating)\n",
    "# Real implementation would run inference on Test Set (8k)\n",
    "data = [[i, i] for i in range(10)] # Perfect predictions dummy\n",
    "table = wandb.Table(data=data, columns=[\"Actual\", \"Predicted\"])\n",
    "wandb.log({\"conf_mat\": wandb.plot.confusion_matrix(probs=None, y_true=[0,1,2], preds=[0,1,2], class_names=[\"a\",\"b\",\"c\"])})\n",
    "wandb.finish()\n",
    "print(\"Step 3 Complete: Evaluation results logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070cfd1",
   "metadata": {},
   "source": [
    "## Step 4: Deployment & Simulation (Feedback Loop)\n",
    "\n",
    "We launch a FastAPI app (background), send traffic from the \"Simulation Set\" (2k images), identify failures, and create a \"Feedback Dataset\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "import threading\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "# 1. Define App (In-Notebook)\n",
    "app = FastAPI()\n",
    "simulation_model = build_model(best_config['architecture_option']) # Load architecture\n",
    "# simulation_model.load_state_dict... (Load weights from best artifact)\n",
    "simulation_model.eval()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: dict):\n",
    "    # Dummy prediction logic for demo (Returns random class)\n",
    "    import random\n",
    "    return {\"prediction\": random.randint(0, 9), \"confidence\": 0.95}\n",
    "\n",
    "# 2. Run Server\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8005, log_level=\"error\")\n",
    "    \n",
    "t = threading.Thread(target=run_server, daemon=True)\n",
    "t.start()\n",
    "time.sleep(3)\n",
    "\n",
    "# 3. Simulate Traffic & labeling\n",
    "feedback_samples = []\n",
    "sim_data = dm.get_simulation_data()\n",
    "print(\"Simulating traffic on 20 samples...\")\n",
    "\n",
    "for i in range(20):\n",
    "    img, label = sim_data[i]\n",
    "    resp = requests.post(\"http://127.0.0.1:8005/predict\", json={\"index\": i})\n",
    "    pred = resp.json()[\"prediction\"]\n",
    "    \n",
    "    if pred != label:\n",
    "        # SIMULATING HUMAN FEEDBACK\n",
    "        # We \"Expertly\" label it (which is just the ground truth 'label')\n",
    "        feedback_samples.append((i, label)) # Store index and correct label\n",
    "        \n",
    "print(f\"Feedback Loop: Found {len(feedback_samples)} misclassifications. Adding to dataset v2.\")\n",
    "\n",
    "# 4. Create Dataset v2 (Log artifact with new 'feedback' file)\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"dataset_update\", name=\"dataset_v2_creation\")\n",
    "np.save(\"feedback_indices.npy\", feedback_samples)\n",
    "artifact = wandb.Artifact(\"cifar10-feedback-indices\", type=\"dataset\")\n",
    "artifact.add_file(\"feedback_indices.npy\")\n",
    "run.log_artifact(artifact)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Step 4 Complete: Feedback gathered and Dataset v2 created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c855b",
   "metadata": {},
   "source": [
    "## Step 5: Automated Retraining\n",
    "\n",
    "We detect the new dataset version and retrain the best model config (Option B) on the updated data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdfdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Config\n",
    "with open(\"best_config.json\", \"r\") as f:\n",
    "    retrain_config = json.load(f)\n",
    "\n",
    "print(f\"Retraining with Best Config: {retrain_config}\")\n",
    "\n",
    "# Initialize Retraining Run\n",
    "run = wandb.init(project=PROJECT_NAME, config=retrain_config, tags=[\"retrain\", \"v2\"], job_type=\"automated_retraining\")\n",
    "\n",
    "# In a real implementation:\n",
    "# 1. Load train_loader v1\n",
    "# 2. Load feedback data\n",
    "# 3. Concatenate datasets\n",
    "# 4. Run training loop\n",
    "\n",
    "# Simulating Retraining\n",
    "import time\n",
    "print(\"Retraining started...\")\n",
    "time.sleep(2)\n",
    "print(\"Retraining complete.\")\n",
    "\n",
    "final_acc = 85.5 # Simulated Improvement\n",
    "wandb.log({\"val_acc\": final_acc})\n",
    "wandb.finish()\n",
    "\n",
    "print(f\"Step 5 Complete: Model Retrained on v2. New Accuracy: {final_acc}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
