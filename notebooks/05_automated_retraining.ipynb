{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5089efb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5089efb8",
        "outputId": "9e14202e-c298-4296-a83a-3c3637f4f98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m [wandb.login()] Changing session credentials to explicit value for https://api.wandb.ai.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import glob\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
        "\n",
        "# ==========================================\n",
        "# 1. Setup\n",
        "# ==========================================\n",
        "WANDB_API_KEY = \"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\"\n",
        "PROJECT_NAME = \"cifar10_mlops_project\"\n",
        "ENTITY = \"esi-sba-dz\"\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e68f497f",
      "metadata": {
        "id": "e68f497f"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2. Helpers (Strict No-Download)\n",
        "# ==========================================\n",
        "class Cifar10DataManager:\n",
        "    def __init__(self, data_dir=\"./data\"):\n",
        "        self.data_dir = data_dir\n",
        "        self.mean = (0.4914, 0.4822, 0.4465)\n",
        "        self.std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "    def get_loader_for_retrain(self, batch_size, architecture_option='standard'):\n",
        "        tf_list = [transforms.ToTensor(), transforms.Normalize(self.mean, self.std)]\n",
        "        train_tf = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, 4)] + tf_list)\n",
        "        if architecture_option == 'upsample':\n",
        "            # Fix: Ensure ToTensor and Normalize are included for upsample too\n",
        "            train_tf = transforms.Compose([transforms.Resize(224), transforms.RandomHorizontalFlip()] + tf_list)\n",
        "\n",
        "        # STRICT: download=False\n",
        "        train_set = torchvision.datasets.CIFAR10(root=self.data_dir, train=True, download=False, transform=train_tf)\n",
        "        return train_set, train_tf\n",
        "\n",
        "    def get_test_loader(self, batch_size, architecture_option='standard'):\n",
        "        tf_list = [transforms.ToTensor(), transforms.Normalize(self.mean, self.std)]\n",
        "        test_tf = transforms.Compose(tf_list)\n",
        "        if architecture_option == 'upsample':\n",
        "             test_tf = transforms.Compose([transforms.Resize(224)] + tf_list)\n",
        "\n",
        "        test_set = torchvision.datasets.CIFAR10(root=self.data_dir, train=False, download=False, transform=test_tf)\n",
        "        indices_path = os.path.join(self.data_dir, \"processed\", \"test_indices.npy\")\n",
        "        if os.path.exists(indices_path):\n",
        "            test_indices = np.load(indices_path)\n",
        "            test_set = Subset(test_set, test_indices)\n",
        "\n",
        "        return DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    def get_simulation_raw(self):\n",
        "        return torchvision.datasets.CIFAR10(root=self.data_dir, train=False, download=False)\n",
        "\n",
        "def build_model(architecture_option='standard'):\n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    if architecture_option == 'modified':\n",
        "        model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        model.maxpool = nn.Identity()\n",
        "    elif architecture_option == 'upsample':\n",
        "        pass\n",
        "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "    return model\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(inputs), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            running_loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return running_loss / len(loader), 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a8eb5fbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "a8eb5fbb",
        "outputId": "96235d61-0ce9-463c-b9c9-9071293460b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260130_133247-p7ohb8su</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/esi-sba-dz/cifar10_mlops_project/runs/p7ohb8su' target=\"_blank\">splendid-dream-21</a></strong> to <a href='https://wandb.ai/esi-sba-dz/cifar10_mlops_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/esi-sba-dz/cifar10_mlops_project' target=\"_blank\">https://wandb.ai/esi-sba-dz/cifar10_mlops_project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/esi-sba-dz/cifar10_mlops_project/runs/p7ohb8su' target=\"_blank\">https://wandb.ai/esi-sba-dz/cifar10_mlops_project/runs/p7ohb8su</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Feedback...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Baseline Data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'cifar10_dataset:latest', 340.26MB. 11 files...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   11 of 11 files downloaded.  \n",
            "Done. 00:00:00.2 (1804.6MB/s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by -summary_metrics.val_acc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Baseline Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning (Exact Replication Mode)...\n",
            "Epoch 1 | Train Loss: 0.3342 | Val Loss: 0.2943 | Val Acc: 89.72%\n",
            "Epoch 2 | Train Loss: 0.2410 | Val Loss: 0.2655 | Val Acc: 90.88%\n",
            "Epoch 3 | Train Loss: 0.1896 | Val Loss: 0.2334 | Val Acc: 91.99%\n",
            "Epoch 4 | Train Loss: 0.1586 | Val Loss: 0.2177 | Val Acc: 92.54%\n",
            "Epoch 5 | Train Loss: 0.1246 | Val Loss: 0.2107 | Val Acc: 93.11%\n",
            "Epoch 6 | Train Loss: 0.1084 | Val Loss: 0.2021 | Val Acc: 93.00%\n",
            "Epoch 7 | Train Loss: 0.0903 | Val Loss: 0.2049 | Val Acc: 93.26%\n",
            "Epoch 8 | Train Loss: 0.0763 | Val Loss: 0.2096 | Val Acc: 93.54%\n",
            "Retraining Complete.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>retrain_loss</td><td>█▅▄▃▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▃▅▆▇▇▇█</td></tr><tr><td>val_loss</td><td>█▆▃▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>retrain_loss</td><td>0.07632</td></tr><tr><td>val_acc</td><td>93.5375</td></tr><tr><td>val_loss</td><td>0.20965</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">splendid-dream-21</strong> at: <a href='https://wandb.ai/esi-sba-dz/cifar10_mlops_project/runs/p7ohb8su' target=\"_blank\">https://wandb.ai/esi-sba-dz/cifar10_mlops_project/runs/p7ohb8su</a><br> View project at: <a href='https://wandb.ai/esi-sba-dz/cifar10_mlops_project' target=\"_blank\">https://wandb.ai/esi-sba-dz/cifar10_mlops_project</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260130_133247-p7ohb8su/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. Execution\n",
        "# ==========================================\n",
        "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"retrain\", tags=[\"retrain\"])\n",
        "\n",
        "try:\n",
        "    print(\"Downloading Feedback...\")\n",
        "    f_art = run.use_artifact(f'{ENTITY}/{PROJECT_NAME}/cifar10-feedback:latest').download(root=\".\")\n",
        "    feedback = np.load(os.path.join(f_art, \"feedback_v1.npy\"))\n",
        "except:\n",
        "    print(\"No feedback found.\")\n",
        "    feedback = []\n",
        "\n",
        "if len(feedback) > 0:\n",
        "    print(\"Downloading Baseline Data...\")\n",
        "    run.use_artifact(f'{ENTITY}/{PROJECT_NAME}/cifar10_dataset:latest').download(\"./data\")\n",
        "\n",
        "    api = wandb.Api()\n",
        "    sweeps = api.project(PROJECT_NAME, entity=ENTITY).sweeps()\n",
        "    best_run = api.sweep(f\"{ENTITY}/{PROJECT_NAME}/{sweeps[0].id}\").best_run()\n",
        "    config = best_run.config\n",
        "\n",
        "    print(\"Downloading Baseline Model...\")\n",
        "    m_dir = best_run.logged_artifacts()[0].download(root=\"./models\")\n",
        "    m_path = glob.glob(os.path.join(m_dir, \"*.pth\"))[0]\n",
        "\n",
        "    # Dataset Merge\n",
        "    dm = Cifar10DataManager()\n",
        "    base_train, tf = dm.get_loader_for_retrain(config['batch_size'], config['architecture_option'])\n",
        "    raw_sim = dm.get_simulation_raw()\n",
        "\n",
        "    # NEW: Get validation loader\n",
        "    test_loader = dm.get_test_loader(config['batch_size'], config['architecture_option'])\n",
        "\n",
        "    class FeedbackDS(torch.utils.data.Dataset):\n",
        "        def __init__(self, raw, inds, tf):\n",
        "             self.raw = raw; self.inds = [int(i[0]) for i in inds]; self.tf = tf\n",
        "        def __len__(self): return len(self.inds)\n",
        "        def __getitem__(self, i):\n",
        "             img, label = self.raw[self.inds[i]]\n",
        "             return self.tf(img), label\n",
        "\n",
        "    fb_ds = FeedbackDS(raw_sim, feedback, tf)\n",
        "    loader = DataLoader(ConcatDataset([base_train, fb_ds]), batch_size=config['batch_size'], shuffle=True)\n",
        "\n",
        "    # Retrain\n",
        "    model = build_model(config['architecture_option']).to(device)\n",
        "    model.load_state_dict(torch.load(m_path, map_location=device))\n",
        "\n",
        "    # NEW: Use config for optimizer to match original training\n",
        "    lr = config.get('learning_rate', 0.001)\n",
        "    if config.get('optimizer') == 'sgd':\n",
        "        opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    else:\n",
        "        opt = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Fine-tuning (Exact Replication Mode)...\")\n",
        "    for e in range(config['epochs']):\n",
        "        train_loss = train_epoch(model, loader, crit, opt, device)\n",
        "        val_loss, val_acc = validate(model, test_loader, crit, device)\n",
        "\n",
        "        print(f\"Epoch {e+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "        wandb.log({\n",
        "            \"retrain_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "            \"epoch\": e\n",
        "        })\n",
        "\n",
        "    torch.save(model.state_dict(), \"retrained.pth\")\n",
        "    art = wandb.Artifact(\"retrained-model\", type=\"model\")\n",
        "    art.add_file(\"retrained.pth\")\n",
        "    run.log_artifact(art)\n",
        "    print(\"Retraining Complete.\")\n",
        "else:\n",
        "    print(\"Skipping.\")\n",
        "\n",
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}