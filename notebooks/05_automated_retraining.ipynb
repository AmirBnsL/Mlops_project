{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5089efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==========================================\n",
    "# 1. Environment & Setup\n",
    "# ==========================================\n",
    "load_dotenv(os.path.join(os.getcwd(), \".env\"))\n",
    "PROJECT_NAME = os.getenv(\"WANDB_PROJECT\", \"cifar10_mlops_project\")\n",
    "ENTITY = os.getenv(\"WANDB_ENTITY\", None)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ==========================================\n",
    "# 2. Shared Code (Inlined)\n",
    "# ==========================================\n",
    "\n",
    "class Cifar10DataManager:\n",
    "    def __init__(self, data_dir=\"./data\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.mean = (0.4914, 0.4822, 0.4465)\n",
    "        self.std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "    def get_transforms(self, architecture_option='standard'):\n",
    "        transform_list = [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.mean, self.std)\n",
    "        ]\n",
    "        train_transforms = [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4)\n",
    "        ] + transform_list\n",
    "\n",
    "        if architecture_option == 'upsample':\n",
    "            transform_list.insert(0, transforms.Resize(224))\n",
    "            train_transforms.insert(0, transforms.Resize(224))\n",
    "\n",
    "        return transforms.Compose(train_transforms), transforms.Compose(transform_list)\n",
    "\n",
    "    def get_loaders(self, batch_size, architecture_option='standard'):\n",
    "        train_transform, test_transform = self.get_transforms(architecture_option)\n",
    "        train_set = torchvision.datasets.CIFAR10(root=self.data_dir, train=True, download=True, transform=train_transform)\n",
    "        test_set = torchvision.datasets.CIFAR10(root=self.data_dir, train=False, download=True, transform=test_transform)\n",
    "        \n",
    "        indices_path = os.path.join(self.data_dir, \"processed\", \"test_indices.npy\")\n",
    "        if not os.path.exists(indices_path):\n",
    "             # Ensure we have data\n",
    "             raise FileNotFoundError(f\"Indices not found at {indices_path}\")\n",
    "             \n",
    "        test_indices = np.load(indices_path)\n",
    "        real_test_set = Subset(test_set, test_indices)\n",
    "        \n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        test_loader = DataLoader(real_test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    def get_simulation_data(self):\n",
    "        test_set_raw = torchvision.datasets.CIFAR10(root=self.data_dir, train=False, download=True, transform=None)\n",
    "        sim_indices = np.load(os.path.join(self.data_dir, \"processed\", \"sim_indices.npy\"))\n",
    "        return Subset(test_set_raw, sim_indices)\n",
    "\n",
    "def build_model(architecture_option='standard', num_classes=10, pretrained=True):\n",
    "    model = torchvision.models.resnet18(pretrained=pretrained)\n",
    "    if architecture_option == 'modified':\n",
    "        model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        model.maxpool = nn.Identity()\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return running_loss / len(loader), 100 * correct / total\n",
    "\n",
    "# ==========================================\n",
    "# 3. Automated Retraining Logic\n",
    "# ==========================================\n",
    "\n",
    "print(\"Checking for Feedback Artifacts...\")\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"automated_retraining\", tags=[\"retrain\"])\n",
    "\n",
    "# 3.1 Download Feedback Data\n",
    "try:\n",
    "    artifact = run.use_artifact(f'{ENTITY}/{PROJECT_NAME}/cifar10-feedback:latest', type='dataset')\n",
    "    artifact_dir = artifact.download(root=\".\")\n",
    "    feedback_path = os.path.join(artifact_dir, \"feedback_v1.npy\")\n",
    "    \n",
    "    if os.path.exists(feedback_path):\n",
    "        feedback_data = np.load(feedback_path)\n",
    "        print(f\"Found {len(feedback_data)} feedback samples to integrate.\")\n",
    "    else:\n",
    "        feedback_data = []\n",
    "except Exception as e:\n",
    "    print(f\"No feedback artifact found. (Normally this means no failures reported yet). Info: {e}\")\n",
    "    feedback_data = []\n",
    "\n",
    "if len(feedback_data) > 0:\n",
    "    # 3.2 Fetch Base Configuration\n",
    "    api = wandb.Api()\n",
    "    \n",
    "    # Resolve Sweep ID\n",
    "    sweep_id = os.getenv(\"SWEEP_ID\")\n",
    "    if not sweep_id:\n",
    "        sweeps = api.project(PROJECT_NAME, entity=ENTITY).sweeps()\n",
    "        if len(sweeps) > 0:\n",
    "            sweep_id = sweeps[0].id\n",
    "            print(f\"Using latest sweep: {sweep_id}\")\n",
    "    \n",
    "    if not sweep_id:\n",
    "        print(\"Skipping: No Sweep ID found.\")\n",
    "        run.finish()\n",
    "        exit()\n",
    "\n",
    "    sweep = api.sweep(f\"{ENTITY}/{PROJECT_NAME}/{sweep_id}\")\n",
    "    best_run = sweep.best_run()\n",
    "    config = best_run.config\n",
    "    \n",
    "    # 3.3 Ensure Dataset Available (Indices needed)\n",
    "    if not os.path.exists(\"./data/processed/sim_indices.npy\"):\n",
    "        print(\"Downloading dataset artifact...\")\n",
    "        run.use_artifact(f'{ENTITY}/{PROJECT_NAME}/cifar10_dataset:latest').download(\"./data\")\n",
    "\n",
    "    dm = Cifar10DataManager(data_dir=\"./data\")\n",
    "    train_loader_base, test_loader = dm.get_loaders(config['batch_size'], config['architecture_option'])\n",
    "    \n",
    "    # 3.4 Build \"Augmented\" Dataset (Train + Feedback)\n",
    "    sim_data_subset = dm.get_simulation_data() # Raw PIL\n",
    "    \n",
    "    class FeedbackDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, subset, feedback_indices, transform=None):\n",
    "            self.subset = subset\n",
    "            self.indices = [int(x[0]) for x in feedback_indices]\n",
    "            self.transform = transform\n",
    "        def __len__(self): return len(self.indices)\n",
    "        def __getitem__(self, idx):\n",
    "            sim_idx = self.indices[idx]\n",
    "            image, label = self.subset[sim_idx]\n",
    "            if self.transform: image = self.transform(image)\n",
    "            return image, label\n",
    "\n",
    "    train_transform, _ = dm.get_transforms(config['architecture_option'])\n",
    "    feedback_ds = FeedbackDataset(sim_data_subset, feedback_data, transform=train_transform)\n",
    "    \n",
    "    full_train_set = ConcatDataset([train_loader_base.dataset, feedback_ds])\n",
    "    full_train_loader = DataLoader(full_train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n",
    "    \n",
    "    print(f\"Retraining on {len(full_train_set)} samples...\")\n",
    "    \n",
    "    # 3.5 Load Base Model Weights\n",
    "    artifacts = best_run.logged_artifacts()\n",
    "    model_artifact = [a for a in artifacts if a.type == \"model\"][0]\n",
    "    model_dir = model_artifact.download(root=\"./models\")\n",
    "    model_path = glob.glob(os.path.join(model_dir, \"*.pth\"))[0]\n",
    "    \n",
    "    model = build_model(config['architecture_option']).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # 3.6 Fine-tune\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(2):\n",
    "        print(f\"Retraining Epoch {epoch+1}...\")\n",
    "        loss = train_epoch(model, full_train_loader, criterion, optimizer, device)\n",
    "        _, acc = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} Loss: {loss:.4f} Acc: {acc:.2f}%\")\n",
    "        wandb.log({\"retrain_loss\": loss, \"retrain_acc\": acc})\n",
    "        \n",
    "    # 3.7 Save New Version\n",
    "    os.makedirs(\"./models\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), \"./models/model_retrained_v2.pth\")\n",
    "    \n",
    "    art = wandb.Artifact(f\"model-retrained-v2\", type=\"model\")\n",
    "    art.add_file(\"./models/model_retrained_v2.pth\")\n",
    "    run.log_artifact(art)\n",
    "    print(\"Retrained model v2 saved and logged.\")\n",
    "    \n",
    "else:\n",
    "    print(\"No feedback data found. Retraining skipped.\")\n",
    "\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
