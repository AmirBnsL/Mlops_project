{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154a82d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import functools\n",
        "import json\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. Setup & Auth\n",
        "# ==========================================\n",
        "WANDB_API_KEY = \"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\"\n",
        "PROJECT_NAME = \"cifar10_mlops_project\"\n",
        "ENTITY = \"esi-sba-dz\"\n",
        "\n",
        "wandb.login(key=WANDB_API_KEY)\n",
        "print(f\"Project: {PROJECT_NAME}, Entity: {ENTITY}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ed26fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2. Data Manager (Artifact-Based)\n",
        "# ==========================================\n",
        "class Cifar10DataManager:\n",
        "    def __init__(self, data_dir=\"./data\"):\n",
        "        self.data_dir = data_dir\n",
        "        self.mean = (0.4914, 0.4822, 0.4465)\n",
        "        self.std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "    def get_transforms(self, architecture_option='standard'):\n",
        "        transform_list = [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(self.mean, self.std)\n",
        "        ]\n",
        "        train_transforms = [\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, padding=4)\n",
        "        ] + transform_list\n",
        "\n",
        "        if architecture_option == 'upsample':\n",
        "            transform_list.insert(0, transforms.Resize(224))\n",
        "            train_transforms.insert(0, transforms.Resize(224))\n",
        "\n",
        "        return transforms.Compose(train_transforms), transforms.Compose(transform_list)\n",
        "\n",
        "    def get_loaders(self, batch_size, architecture_option='standard'):\n",
        "        train_transform, test_transform = self.get_transforms(architecture_option)\n",
        "        \n",
        "        # KEY CHANGE: download=False\n",
        "        # We expect data to be present via W&B Artifact download.\n",
        "        # If this fails, it means we didn't get the artifact correctly.\n",
        "        try:\n",
        "            train_set = torchvision.datasets.CIFAR10(root=self.data_dir, train=True, download=False, transform=train_transform)\n",
        "            test_set = torchvision.datasets.CIFAR10(root=self.data_dir, train=False, download=False, transform=test_transform)\n",
        "        except RuntimeError:\n",
        "            print(\"CRITICAL: Data not found locally. Ensure Artifact is downloaded first.\")\n",
        "            raise\n",
        "\n",
        "        # Load indices\n",
        "        indices_path = os.path.join(self.data_dir, \"processed\", \"test_indices.npy\")\n",
        "        if not os.path.exists(indices_path):\n",
        "             raise FileNotFoundError(f\"Indices file missing: {indices_path}\")\n",
        "             \n",
        "        test_indices = np.load(indices_path)\n",
        "        real_test_set = Subset(test_set, test_indices)\n",
        "        \n",
        "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        test_loader = DataLoader(real_test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "        \n",
        "        return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c745bdd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. Model & Training Logic\n",
        "# ==========================================\n",
        "def build_model(architecture_option='standard', num_classes=10, pretrained=True):\n",
        "    model = torchvision.models.resnet18(pretrained=pretrained)\n",
        "    if architecture_option == 'modified':\n",
        "        model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        model.maxpool = nn.Identity()\n",
        "    elif architecture_option == 'upsample':\n",
        "        # Upsample happens in transforms; model remains standard\n",
        "        pass\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(inputs), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            running_loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return running_loss / len(loader), 100 * correct / total\n",
        "\n",
        "def run_training_sweep(config=None, data_dir=\"./data\"):\n",
        "    with wandb.init(config=config, entity=ENTITY, project=PROJECT_NAME):\n",
        "        cfg = wandb.config\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        dm = Cifar10DataManager(data_dir=data_dir)\n",
        "        train_loader, test_loader = dm.get_loaders(cfg.batch_size, cfg.architecture_option)\n",
        "        model = build_model(cfg.architecture_option).to(device)\n",
        "        \n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=cfg.learning_rate, momentum=0.9) if cfg.optimizer == \"sgd\" else optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
        "            \n",
        "        best_acc = 0.0\n",
        "        for epoch in range(cfg.epochs):\n",
        "            train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "            val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
        "            \n",
        "            wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
        "            \n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                os.makedirs(\"models\", exist_ok=True)\n",
        "                model_path = f\"models/model_{wandb.run.id}.pth\"\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                \n",
        "                art = wandb.Artifact(f\"model-{wandb.run.id}\", type=\"model\")\n",
        "                art.add_file(model_path)\n",
        "                wandb.log_artifact(art)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "380460f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 4. Download Source Data (Artifact)\n",
        "# ==========================================\n",
        "print(\"Fetch Data Artifact (Pre-Sweep)...\")\n",
        "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"training_prep\")\n",
        "artifact = run.use_artifact(f'{ENTITY}/{PROJECT_NAME}/cifar10_dataset:latest', type='dataset')\n",
        "artifact_dir = artifact.download(root=\"./data\")\n",
        "run.finish()\n",
        "print(f\"Data verified at {artifact_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d093590c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 5. Execute Sweep\n",
        "# ==========================================\n",
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'name': 'val_acc', 'goal': 'maximize'},\n",
        "    'parameters': {\n",
        "        'learning_rate': {'min': 0.001, 'max': 0.1},\n",
        "        'batch_size': {'values': [64, 128]},\n",
        "        'optimizer': {'values': ['adam', 'sgd']},\n",
        "        'architecture_option': {'values': ['standard', 'upsample', 'modified']},\n",
        "        'epochs': {'value': 5}\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME, entity=ENTITY)\n",
        "print(f\"Sweep ID: {sweep_id}\")\n",
        "\n",
        "# Run Agent\n",
        "train_func = functools.partial(run_training_sweep, data_dir=\"./data\")\n",
        "wandb.agent(sweep_id, train_func, count=5, project=PROJECT_NAME, entity=ENTITY)\n",
        "\n",
        "# Save Best Config\n",
        "api = wandb.Api()\n",
        "best_run = api.sweep(f\"{ENTITY}/{PROJECT_NAME}/{sweep_id}\").best_run()\n",
        "with open(\"artifacts/best_config.json\", \"w\") as f:\n",
        "    json.dump(best_run.config, f)\n",
        "print(\"Sweep Complete.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}