{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a350a38d",
   "metadata": {},
   "source": [
    "# Model Training and Hyperparameter Sweep\n",
    "\n",
    "This notebook implements the training pipeline with Transfer Learning (ResNet18) and uses W&B Sweeps for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "import os\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "# setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sweep Configuration\n",
    "sweep_config = {\n",
    "    'method': 'bayes', # Bayesian optimization\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'min': 0.0001,\n",
    "            'max': 0.1\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'sgd']\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 5 # Keep it small for demonstration, increase for real results\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "PROJECT_NAME = \"cifar10_mlops_project\"\n",
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44699265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(batch_size):\n",
    "    # Data Augmentation and Normalization\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='../data/raw', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='../data/raw', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "def build_model():\n",
    "    # Load pretrained ResNet18\n",
    "    model = torchvision.models.resnet18(pretrained=True)\n",
    "    \n",
    "    # Freeze initial layers (optional, but good for transfer learning on small data)\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "        \n",
    "    # Replace last layer for CIFAR-10 (10 classes)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 10)\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        trainloader, testloader = build_dataset(config.batch_size)\n",
    "        model = build_model()\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        if config.optimizer == \"sgd\":\n",
    "            optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
    "        elif config.optimizer == \"adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "            \n",
    "        best_acc = 0.0\n",
    "        \n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for i, (inputs, labels) in enumerate(trainloader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # Log batch metrics\n",
    "                wandb.log({\"batch_loss\": loss.item()})\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = 100 * correct / total\n",
    "            epoch_loss = running_loss / len(trainloader)\n",
    "            \n",
    "            # Log epoch metrics\n",
    "            wandb.log({\"epoch\": epoch, \"loss\": epoch_loss, \"val_acc\": val_acc})\n",
    "            print(f\"Epoch {epoch}: Loss {epoch_loss:.3f}, Val Acc {val_acc:.2f}%\")\n",
    "            \n",
    "            # Save best model to W&B\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                \n",
    "                # Create models directory\n",
    "                Path(\"../models\").mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # Save locally\n",
    "                model_path = f\"../models/model_best_{wandb.run.id}.pth\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                \n",
    "                # Log as artifact\n",
    "                artifact = wandb.Artifact(f\"model-best-{wandb.run.id}\", type=\"model\")\n",
    "                artifact.add_file(model_path)\n",
    "                wandb.log_artifact(artifact)\n",
    "                print(f\"New best model saved with acc: {best_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Sweep Agent\n",
    "# count=5 means run 5 experiments\n",
    "wandb.agent(sweep_id, train, count=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
