{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb620cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and splitting data...\n",
      "Downloading/Loading data in ./data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:02<00:00, 74.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split indices created.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ==========================================\n",
    "# 1. Environment Setup (Standalone)\n",
    "# ==========================================\n",
    "load_dotenv(os.path.join(os.getcwd(), \".env\"))\n",
    "\n",
    "PROJECT_NAME = os.getenv(\"WANDB_PROJECT\", \"cifar10_mlops_project\")\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "if not WANDB_API_KEY:\n",
    "    print(\"WANDB_API_KEY not found in .env. Please login manually.\")\n",
    "    wandb.login()\n",
    "\n",
    "# ==========================================\n",
    "# 2. Shared Code (Inlined from src/dataset.py)\n",
    "# ==========================================\n",
    "class Cifar10DataManager:\n",
    "    def __init__(self, data_dir=\"./data\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.mean = (0.4914, 0.4822, 0.4465)\n",
    "        self.std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "    def get_transforms(self, architecture_option='standard'):\n",
    "        # Base transforms\n",
    "        transform_list = [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(self.mean, self.std)\n",
    "        ]\n",
    "        \n",
    "        train_transforms = [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4)\n",
    "        ] + transform_list\n",
    "\n",
    "        if architecture_option == 'upsample':\n",
    "            transform_list.insert(0, transforms.Resize(224))\n",
    "            train_transforms.insert(0, transforms.Resize(224))\n",
    "\n",
    "        return transforms.Compose(train_transforms), transforms.Compose(transform_list)\n",
    "\n",
    "    def prepare_initial_split(self):\n",
    "        \"\"\"\n",
    "        Downloads CIFAR-10.\n",
    "        Splits Test set (10k) into:\n",
    "        - Test (8k): For model evaluation\n",
    "        - Simulation (2k): For live traffic simulation (Holdout)\n",
    "        \"\"\"\n",
    "        print(f\"Downloading/Loading data in {self.data_dir}...\")\n",
    "        # Download raw data\n",
    "        train_set = torchvision.datasets.CIFAR10(root=self.data_dir, train=True, download=True)\n",
    "        test_set = torchvision.datasets.CIFAR10(root=self.data_dir, train=False, download=True)\n",
    "        \n",
    "        # Split Test Set\n",
    "        indices = list(range(len(test_set)))\n",
    "        # Shuffle deterministically for reproducibility\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        test_indices = indices[:8000]\n",
    "        sim_indices = indices[8000:]\n",
    "        \n",
    "        # Save indices to disk to ensure we load the same split later\n",
    "        processed_dir = os.path.join(self.data_dir, \"processed\")\n",
    "        os.makedirs(processed_dir, exist_ok=True)\n",
    "        np.save(os.path.join(processed_dir, \"test_indices.npy\"), test_indices)\n",
    "        np.save(os.path.join(processed_dir, \"sim_indices.npy\"), sim_indices)\n",
    "        print(\"Data split indices created.\")\n",
    "        \n",
    "        return train_set, test_set, test_indices, sim_indices\n",
    "\n",
    "# ==========================================\n",
    "# 3. Execution Main\n",
    "# ==========================================\n",
    "\n",
    "# Initialize Data Manager\n",
    "dm = Cifar10DataManager(data_dir=\"./data\")\n",
    "\n",
    "# 1. Download & Prepare Initial Split\n",
    "# This downloads CIFAR-10 from Torchvision and creates the 40k/8k/2k split indices locally\n",
    "print(\"Downloading and splitting data...\")\n",
    "dm.prepare_initial_split()\n",
    "\n",
    "# 2. Versioning with W&B\n",
    "# We create a new run to log this dataset as the \"Source of truth\"\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"data_preparation\", name=\"cifar10_v1\")\n",
    "\n",
    "# We create an artifact that contains the entire data directory (Raw images + Split Indices)\n",
    "dataset_artifact = wandb.Artifact(\n",
    "    name=\"cifar10_dataset\", \n",
    "    type=\"dataset\", \n",
    "    description=\"CIFAR-10 Raw Data + Split Indices (Train/Test/Sim)\"\n",
    ")\n",
    "\n",
    "# Add the data directory to the artifact\n",
    "# Note: This uploads the whole ./data folder including the 'cifar-10-batches-py' and 'processed'\n",
    "dataset_artifact.add_dir(\"./data\")\n",
    "\n",
    "# Log it\n",
    "run.log_artifact(dataset_artifact)\n",
    "run.finish()\n",
    "\n",
    "print(\"Step 1 Complete: Dataset v1 logged to W&B.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce43fff",
   "metadata": {},
   "source": [
    "# Data Preparation and Versioning\n",
    "\n",
    "This notebook downloads the CIFAR-10 dataset and versions it using Weights & Biases Artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21d8587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "# Project Configuration\n",
    "PROJECT_NAME = \"cifar10_mlops_project\"\n",
    "ENTITY = None # Set this to your username if needed, usually inferred\n",
    "ARTIFACT_NAME = \"cifar10-raw-data\"\n",
    "DATA_DIR = \"../data/raw\"\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f008f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize W&B Run for Data Preparation\n",
    "run = wandb.init(project=PROJECT_NAME, job_type=\"data-preparation\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAR-10 Dataset\n",
    "print(\"Downloading CIFAR-10 dataset...\")\n",
    "# We use torchvision to download, it creates a folder 'cifar-10-batches-py' inside DATA_DIR\n",
    "dataset = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True)\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a W&B Artifact\n",
    "artifact = wandb.Artifact(name=ARTIFACT_NAME, type=\"dataset\", description=\"Raw CIFAR-10 dataset from torchvision\")\n",
    "\n",
    "# Add the directory containing the dataset to the artifact\n",
    "# Torchvision CIFAR10 extracts to a folder inside root, usually. \n",
    "# Let's add the whole DATA_DIR content to be sure we capture it.\n",
    "artifact.add_dir(DATA_DIR)\n",
    "\n",
    "# Log the artifact to W&B\n",
    "print(\"Logging artifact to W&B...\")\n",
    "run.log_artifact(artifact)\n",
    "print(\"Artifact logged successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
